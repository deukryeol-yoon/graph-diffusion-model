{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Auto Encoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper Parameters"
      ],
      "metadata": {
        "id": "VGAKM8KSbY_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### torch_geometric 설치를 위한 cell. 없이도 torch geometric이 잘 된다면 삭제해도 무방하다고 생각.\n",
        "\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqsI4szpfCjd",
        "outputId": "f3610c86-93e3-40a9-e40d-cd6d371f2475"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 2.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=73888e04c9173aad5547afe95f6aafe420986e8ffc7d059584179b0cbda68d44\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.modules.loss\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wzbyC6nebbA2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model', type=str, default='gcn_ae', help=\"models used\")\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
        "parser.add_argument('--hidden', type=int, default=32, help='Number of units in hidden layer 1.')\n",
        "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
        "parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--dataset', type=str, default='cora', help='type of dataset.')\n",
        "parser.add_argument('--feat-dim', type=int, default=10000, help='Maximum Dimension of Node Features')\n",
        "parser.add_argument('--max-num-nodes', type=int, default=100000, help='Maximum Dimension of Node Features')\n",
        "parser.add_argument(\"-f\")\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "xbQ2Kk8Jyv0I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1amfNKYhbO2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "R5kcaY4ugALV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for data_name in ['cora','citeseer','pubmed']:\n",
        "  args.dataset = data_name\n",
        "  data = Planetoid(root=f'/tmp/{args.dataset}', name=args.dataset)\n",
        "  data = data[0].to(device)\n",
        "  dataset.append(data)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M5CzWyoeQPe",
        "outputId": "03d271c8-38fc-49e1-d64c-a7a228c00c84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]), Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327]), Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = [i for i in range(0,1)]      ### train = [cora]\n",
        "val_mask = [i for i in range(1,2)]        ### valid = [citeseer]\n",
        "test_mask = [i for i in range(2,3)]       ### test = [pubmed]"
      ],
      "metadata": {
        "id": "g2va8l9EsLMK"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1d = (0, args.feat_dim)\n",
        "def zero_padding_features(input):\n",
        "  p1d = (0,args.feat_dim - input.shape[1])\n",
        "  return F.pad(input,p1d,\"constant\",0)\n",
        "\n",
        "train_dataset = [(zero_padding_features(dataset[i].x), dataset[i].edge_index) for i in train_mask]\n",
        "valid_dataset = [(zero_padding_features(dataset[i].x), dataset[i].edge_index) for i in val_mask]\n",
        "test_dataset = [(zero_padding_features(dataset[i].x), dataset[i].edge_index) for i in test_mask]\n",
        "\n",
        "dataloaders = {}\n",
        "dataloaders['train'] = train_dataset\n",
        "dataloaders['valid'] = valid_dataset\n",
        "dataloaders['test'] = test_dataset"
      ],
      "metadata": {
        "id": "wHrnHTD5bTS8"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model "
      ],
      "metadata": {
        "id": "QxCnEvDEbm6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModelAE(nn.Module):\n",
        "    def __init__(self, input_feat_dim, hidden_dim, dropout):\n",
        "        super(GCNModelAE, self).__init__()\n",
        "        self.gc = GCNConv(input_feat_dim, hidden_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        output = self.gc(x, adj)\n",
        "        output = F.leaky_relu(output, 0.2)\n",
        "        output = F.dropout(output, p = self.dropout, training=self.training)\n",
        "        output = F.sigmoid(torch.mm(output, output.t()))\n",
        "        return output"
      ],
      "metadata": {
        "id": "STeMsV4Tbpse"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss func & Optimizer"
      ],
      "metadata": {
        "id": "BXFsWtqOb78G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNModelAE(args.feat_dim, args.hidden, args.dropout).to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= args.lr)"
      ],
      "metadata": {
        "id": "Od4ZGvZMb9iE"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Bm93QcsKnD6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,dataloaders,loss_func,optimizer):\n",
        "  train_loss_history = []\n",
        "  valid_loss_history = []\n",
        "  for epoch in range(args.epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, args.epochs - 1))\n",
        "    print('-' * 10)\n",
        "    for phase in ['train','valid']:\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "      \n",
        "      cur_loss = 0.0\n",
        "\n",
        "      for node_features, edge_index in dataloaders[phase]:\n",
        "        node_features.to(device)\n",
        "        edge_index.to(device)\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          output = model(node_features,edge_index)\n",
        "          loss = loss_func(output, to_dense_adj(edge_index))  \n",
        "\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        cur_loss += loss.item()\n",
        "      \n",
        "      if phase == 'train':\n",
        "        train_loss_history.append(cur_loss / len(dataloaders[\"train\"]))\n",
        "      if phase == 'valid':\n",
        "        valid_loss_history.append(cur_loss / len(dataloaders[\"valid\"]))\n",
        "\n",
        "      print('{} Loss: {:.4f}'.format(phase, cur_loss))\n",
        "    print()\n",
        "  \n",
        "  return model, train_loss_history, valid_loss_history"
      ],
      "metadata": {
        "id": "lpoLOJ15nFFC"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss_history, valid_loss_history = train_model(model,dataloaders,loss_func,optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7yf5QrcIUsN",
        "outputId": "7557f3cc-244f-4f85-bd5f-57b70eb611a7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "train Loss: 0.2525\n",
            "valid Loss: 0.2502\n",
            "\n",
            "Epoch 1/199\n",
            "----------\n",
            "train Loss: 0.2827\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 2/199\n",
            "----------\n",
            "train Loss: 0.2707\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 3/199\n",
            "----------\n",
            "train Loss: 0.2658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1, 3327, 3327])) that is different to the input size (torch.Size([3327, 3327])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1, 19717, 19717])) that is different to the input size (torch.Size([19717, 19717])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid Loss: 0.2502\n",
            "\n",
            "Epoch 4/199\n",
            "----------\n",
            "train Loss: 0.2719\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 5/199\n",
            "----------\n",
            "train Loss: 0.2656\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 6/199\n",
            "----------\n",
            "train Loss: 0.2619\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 7/199\n",
            "----------\n",
            "train Loss: 0.2642\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 8/199\n",
            "----------\n",
            "train Loss: 0.2710\n",
            "valid Loss: 0.2502\n",
            "\n",
            "Epoch 9/199\n",
            "----------\n",
            "train Loss: 0.2754\n",
            "valid Loss: 0.2502\n",
            "\n",
            "Epoch 10/199\n",
            "----------\n",
            "train Loss: 0.2765\n",
            "valid Loss: 0.2502\n",
            "\n",
            "Epoch 11/199\n",
            "----------\n",
            "train Loss: 0.2747\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 12/199\n",
            "----------\n",
            "train Loss: 0.2703\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 13/199\n",
            "----------\n",
            "train Loss: 0.2650\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 14/199\n",
            "----------\n",
            "train Loss: 0.2614\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 15/199\n",
            "----------\n",
            "train Loss: 0.2564\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 16/199\n",
            "----------\n",
            "train Loss: 0.2552\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 17/199\n",
            "----------\n",
            "train Loss: 0.2536\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 18/199\n",
            "----------\n",
            "train Loss: 0.2543\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 19/199\n",
            "----------\n",
            "train Loss: 0.2571\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 20/199\n",
            "----------\n",
            "train Loss: 0.2551\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 21/199\n",
            "----------\n",
            "train Loss: 0.2527\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 22/199\n",
            "----------\n",
            "train Loss: 0.2535\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 23/199\n",
            "----------\n",
            "train Loss: 0.2543\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 24/199\n",
            "----------\n",
            "train Loss: 0.2547\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 25/199\n",
            "----------\n",
            "train Loss: 0.2550\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 26/199\n",
            "----------\n",
            "train Loss: 0.2548\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 27/199\n",
            "----------\n",
            "train Loss: 0.2543\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 28/199\n",
            "----------\n",
            "train Loss: 0.2534\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 29/199\n",
            "----------\n",
            "train Loss: 0.2527\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 30/199\n",
            "----------\n",
            "train Loss: 0.2519\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 31/199\n",
            "----------\n",
            "train Loss: 0.2512\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 32/199\n",
            "----------\n",
            "train Loss: 0.2510\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 33/199\n",
            "----------\n",
            "train Loss: 0.2511\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 34/199\n",
            "----------\n",
            "train Loss: 0.2514\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 35/199\n",
            "----------\n",
            "train Loss: 0.2513\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 36/199\n",
            "----------\n",
            "train Loss: 0.2509\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 37/199\n",
            "----------\n",
            "train Loss: 0.2507\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 38/199\n",
            "----------\n",
            "train Loss: 0.2508\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 39/199\n",
            "----------\n",
            "train Loss: 0.2509\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 40/199\n",
            "----------\n",
            "train Loss: 0.2509\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 41/199\n",
            "----------\n",
            "train Loss: 0.2509\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 42/199\n",
            "----------\n",
            "train Loss: 0.2508\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 43/199\n",
            "----------\n",
            "train Loss: 0.2507\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 44/199\n",
            "----------\n",
            "train Loss: 0.2505\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 45/199\n",
            "----------\n",
            "train Loss: 0.2503\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 46/199\n",
            "----------\n",
            "train Loss: 0.2503\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 47/199\n",
            "----------\n",
            "train Loss: 0.2503\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 48/199\n",
            "----------\n",
            "train Loss: 0.2503\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 49/199\n",
            "----------\n",
            "train Loss: 0.2503\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 50/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 51/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 52/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 53/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 54/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 55/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 56/199\n",
            "----------\n",
            "train Loss: 0.2502\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 57/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 58/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 59/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 60/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 61/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 62/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 63/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 64/199\n",
            "----------\n",
            "train Loss: 0.2501\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 65/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 66/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 67/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 68/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 69/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 70/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 71/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 72/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 73/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 74/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 75/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 76/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 77/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 78/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 79/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 80/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 81/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 82/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 83/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 84/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 85/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 86/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 87/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 88/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 89/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 90/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 91/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 92/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 93/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 94/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 95/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 96/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 97/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 98/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 99/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 100/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 101/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 102/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 103/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 104/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 105/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 106/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 107/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 108/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 109/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 110/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 111/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 112/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 113/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 114/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 115/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 116/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 117/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 118/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 119/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 120/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 121/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 122/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 123/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 124/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 125/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 126/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 127/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 128/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 129/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 130/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 131/199\n",
            "----------\n",
            "train Loss: 0.2500\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 132/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 133/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 134/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 135/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 136/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 137/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 138/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 139/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 140/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 141/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 142/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 143/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 144/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 145/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 146/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 147/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 148/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 149/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 150/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 151/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 152/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 153/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 154/199\n",
            "----------\n",
            "train Loss: 0.2499\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 155/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 156/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 157/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 158/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 159/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 160/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 161/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 162/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 163/199\n",
            "----------\n",
            "train Loss: 0.2498\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 164/199\n",
            "----------\n",
            "train Loss: 0.2497\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 165/199\n",
            "----------\n",
            "train Loss: 0.2497\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 166/199\n",
            "----------\n",
            "train Loss: 0.2497\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 167/199\n",
            "----------\n",
            "train Loss: 0.2497\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 168/199\n",
            "----------\n",
            "train Loss: 0.2497\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 169/199\n",
            "----------\n",
            "train Loss: 0.2497\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 170/199\n",
            "----------\n",
            "train Loss: 0.2496\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 171/199\n",
            "----------\n",
            "train Loss: 0.2496\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 172/199\n",
            "----------\n",
            "train Loss: 0.2496\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 173/199\n",
            "----------\n",
            "train Loss: 0.2496\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 174/199\n",
            "----------\n",
            "train Loss: 0.2495\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 175/199\n",
            "----------\n",
            "train Loss: 0.2495\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 176/199\n",
            "----------\n",
            "train Loss: 0.2495\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 177/199\n",
            "----------\n",
            "train Loss: 0.2494\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 178/199\n",
            "----------\n",
            "train Loss: 0.2494\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 179/199\n",
            "----------\n",
            "train Loss: 0.2494\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 180/199\n",
            "----------\n",
            "train Loss: 0.2493\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 181/199\n",
            "----------\n",
            "train Loss: 0.2493\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 182/199\n",
            "----------\n",
            "train Loss: 0.2493\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 183/199\n",
            "----------\n",
            "train Loss: 0.2492\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 184/199\n",
            "----------\n",
            "train Loss: 0.2492\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 185/199\n",
            "----------\n",
            "train Loss: 0.2491\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 186/199\n",
            "----------\n",
            "train Loss: 0.2491\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 187/199\n",
            "----------\n",
            "train Loss: 0.2490\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 188/199\n",
            "----------\n",
            "train Loss: 0.2490\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 189/199\n",
            "----------\n",
            "train Loss: 0.2489\n",
            "valid Loss: 0.2500\n",
            "\n",
            "Epoch 190/199\n",
            "----------\n",
            "train Loss: 0.2488\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 191/199\n",
            "----------\n",
            "train Loss: 0.2488\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 192/199\n",
            "----------\n",
            "train Loss: 0.2487\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 193/199\n",
            "----------\n",
            "train Loss: 0.2486\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 194/199\n",
            "----------\n",
            "train Loss: 0.2486\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 195/199\n",
            "----------\n",
            "train Loss: 0.2485\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 196/199\n",
            "----------\n",
            "train Loss: 0.2484\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 197/199\n",
            "----------\n",
            "train Loss: 0.2484\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 198/199\n",
            "----------\n",
            "train Loss: 0.2483\n",
            "valid Loss: 0.2501\n",
            "\n",
            "Epoch 199/199\n",
            "----------\n",
            "train Loss: 0.2482\n",
            "valid Loss: 0.2501\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis = [i for i in range(args.epochs)]\n",
        "plt.plot(train_loss_history, label='train')\n",
        "plt.plot(valid_loss_history, label='val')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "V4MOTvQ9Idw0",
        "outputId": "2e454602-48da-4750-8d8f-dd011e1ff50a"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "8vtz0MAG4weG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    cur_loss = 0.0\n",
        "    for node_features, edge_index in dataloaders[\"test\"]:\n",
        "        node_features = node_features.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "\n",
        "        outputs = model(node_features,edge_index)\n",
        "        test_loss = loss_func(outputs, to_dense_adj(edge_index))\n",
        "        \n",
        "        cur_loss += test_loss.item()\n",
        "\n",
        "    test_loss = cur_loss / len(dataloaders[\"test\"])\n",
        "    print(test_loss)     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrOxOHFE1yy0",
        "outputId": "d617ffad-2cf6-4fd4-e110-b821b73aec2c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2519385814666748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1, 2708, 2708])) that is different to the input size (torch.Size([2708, 2708])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    }
  ]
}