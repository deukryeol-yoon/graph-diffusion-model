# -*- coding: utf-8 -*-
"""Graph Auto Encoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16yZvhJB7gmOpH0RWJg4vWr5jSwjiXjyB

## Hyper Parameters
"""

### torch_geometric 설치를 위한 cell. 없이도 torch geometric이 잘 된다면 삭제해도 무방하다고 생각.

import torch

def format_pytorch_version(version):
  return version.split('+')[0]

TORCH_version = torch.__version__
TORCH = format_pytorch_version(TORCH_version)

def format_cuda_version(version):
  return 'cu' + version.replace('.', '')

CUDA_version = torch.version.cuda
CUDA = format_cuda_version(CUDA_version)

!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-geometric

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.nn.modules.module import Module
from torch.nn.parameter import Parameter
import torch.nn.modules.loss
from torch_geometric.nn import GCNConv
from torch_geometric.datasets import Planetoid
from torch_geometric.utils import to_dense_adj
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
import argparse
import time
import numpy as np
import scipy.sparse as sp
import matplotlib.pyplot as plt

parser = argparse.ArgumentParser()
parser.add_argument('--model', type=str, default='gcn_ae', help="models used")
parser.add_argument('--seed', type=int, default=42, help='Random seed.')
parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')
parser.add_argument('--hidden', type=int, default=32, help='Number of units in hidden layer 1.')
parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')
parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')
parser.add_argument('--dataset', type=str, default='cora', help='type of dataset.')
parser.add_argument('--feat-dim', type=int, default=10000, help='Maximum Dimension of Node Features')
parser.add_argument('--max-num-nodes', type=int, default=100000, help='Maximum Dimension of Node Features')
parser.add_argument("-f")

args = parser.parse_args()

"""## Data"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

dataset = []
for data_name in ['cora','citeseer','pubmed']:
  args.dataset = data_name
  data = Planetoid(root=f'/tmp/{args.dataset}', name=args.dataset)
  data = data[0].to(device)
  dataset.append(data)

print(dataset)

train_mask = [i for i in range(0,1)]      ### train = [cora]
val_mask = [i for i in range(1,2)]        ### valid = [citeseer]
test_mask = [i for i in range(2,3)]       ### test = [pubmed]

p1d = (0, args.feat_dim)
def zero_padding_features(input):
  p1d = (0,args.feat_dim - input.shape[1])
  return F.pad(input,p1d,"constant",0)

train_dataset = [(zero_padding_features(dataset[i].x), dataset[i].edge_index) for i in train_mask]
valid_dataset = [(zero_padding_features(dataset[i].x), dataset[i].edge_index) for i in val_mask]
test_dataset = [(zero_padding_features(dataset[i].x), dataset[i].edge_index) for i in test_mask]

dataloaders = {}
dataloaders['train'] = train_dataset
dataloaders['valid'] = valid_dataset
dataloaders['test'] = test_dataset

"""## Model """

class GCNModelAE(nn.Module):
    def __init__(self, input_feat_dim, hidden_dim, dropout):
        super(GCNModelAE, self).__init__()
        self.gc = GCNConv(input_feat_dim, hidden_dim)
        self.dropout = dropout

    def forward(self, x, adj):
        output = self.gc(x, adj)
        output = F.leaky_relu(output, 0.2)
        output = F.dropout(output, p = self.dropout, training=self.training)
        output = F.sigmoid(torch.mm(output, output.t()))
        return output

"""## Loss func & Optimizer"""

model = GCNModelAE(args.feat_dim, args.hidden, args.dropout).to(device)
loss_func = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr= args.lr)

"""## Train"""

def train_model(model,dataloaders,loss_func,optimizer):
  train_loss_history = []
  valid_loss_history = []
  for epoch in range(args.epochs):
    print('Epoch {}/{}'.format(epoch, args.epochs - 1))
    print('-' * 10)
    for phase in ['train','valid']:
      if phase == 'train':
        model.train()
      else:
        model.eval()
      
      cur_loss = 0.0

      for node_features, edge_index in dataloaders[phase]:
        node_features.to(device)
        edge_index.to(device)
      
        optimizer.zero_grad()
        with torch.set_grad_enabled(phase == 'train'):
          output = model(node_features,edge_index)
          loss = loss_func(output, to_dense_adj(edge_index))  

          if phase == 'train':
            loss.backward()
            optimizer.step()

        cur_loss += loss.item()
      
      if phase == 'train':
        train_loss_history.append(cur_loss / len(dataloaders["train"]))
      if phase == 'valid':
        valid_loss_history.append(cur_loss / len(dataloaders["valid"]))

      print('{} Loss: {:.4f}'.format(phase, cur_loss))
    print()
  
  return model, train_loss_history, valid_loss_history

model, train_loss_history, valid_loss_history = train_model(model,dataloaders,loss_func,optimizer)

x_axis = [i for i in range(args.epochs)]
plt.plot(train_loss_history, label='train')
plt.plot(valid_loss_history, label='val')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()

"""## Test"""

with torch.no_grad():
    cur_loss = 0.0
    for node_features, edge_index in dataloaders["test"]:
        node_features = node_features.to(device)
        edge_index = edge_index.to(device)

        outputs = model(node_features,edge_index)
        test_loss = loss_func(outputs, to_dense_adj(edge_index))
        
        cur_loss += test_loss.item()

    test_loss = cur_loss / len(dataloaders["test"])
    print(test_loss)